# Awesome Cross-/Multi-Lingual Cross-Modal Retrieval 


## Table of Contents
* [Datasets](#datasets)
  * [Image-Text](#image-text)
  * [Video-Text](#video-text)
* [Papers and Code](#papers-and-code)
  * [2024 Papers](#2024)
  * [2023 Papers](#2023)
  * [2022 Papers](#2022)
  * [2021 Papers](#2021)
  * [2020 Papers](#2020)
  * [2019 Papers](#2019)
<br></br>

## Datasets
### Image-Text
1. `[ACL-16]` **Multi30K**(multi-lingual version of Filickr30K)-[English|German|French|Czech]: Multi30K: Multilingual English-German Image Descriptions. [[paper]](https://aclanthology.org/W16-3210.pdf) [[dataset]](https://github.com/multi30k/dataset)
2.  **MSCOCO**-[English|Chinese|Japanese]: 
  - (English) `[ARXIV-15]` Microsoft COCO Captions: Data Collection and Evaluation Server. [[paper]](https://arxiv.org/abs/1504.00325) [[dataset]](https://cocodataset.org/#home)
  - (Chinese) `[TMM-19]` COCO-CN for Cross-Lingual Image Tagging, Captioning and Retrieval. [[paper]](https://arxiv.org/pdf/1805.08661.pdf) [[dataset]](https://github.com/li-xirong/coco-cn)
  - (Japanese) `[ACL-17]` STAIR Captions:Constructing a Large-Scale Japanese Image Caption Dataset. [[paper]](https://arxiv.org/pdf/1705.00823v1.pdf) [[dataset]](https://github.com/STAIR-Lab-CIT/STAIR-captions)
3. **CC3M**(mutli-lingual version) [[dataset]](https://github.com/zmykevin/UC2)
4. **Wukong**(Chinese) [[dataset]](https://wukong-dataset.github.io/wukong-dataset/)
### Video-Text
1. `[ICCV-19]` **VATEX**-[English|Chinese]: VATEX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research. [[Paper]](https://arxiv.org/abs/1904.03493) [[dataset]](https://eric-xw.github.io/vatex-website/about.html)
2. `[ACM MM-22]` **MSRVTT-CN**(multi-lingual version of MSRVTT)-[English|Chinese]: Cross-Lingual Cross-Modal Retrieval with Noise-Robust Learning. [[paper]](https://arxiv.org/abs/2208.12526) [[dataset]](https://arxiv.org/abs/2208.12526)

Note: This [repository](https://arxiv.org/abs/2208.12526) provides English captions and other language(Machine-translation version) captions of Multi30K, MSCOCO, VATEX, and MSRVTT-CN.

<br></br>

## Papers and Code
### 2024
- `[Wang et al. AAAI]` CL2CM: Improving Cross-Lingual Cross-Modal Retrieval via Cross-Lingual Knowledge Transfer. [[paper]](https://arxiv.org/pdf/2312.08984.pdf)
### 2023
- `[Zeng et al. ACL]` Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal Pre-training. [[paper]](https://arxiv.org/abs/2206.00621)  [[code]](https://github.com/zengyan-97/CCLM)
- `[Li et al. ACL]` Unifying Cross-Lingual and Cross-Modal Modeling Towards Weakly Supervised Multilingual Vision-Language Pre-training. [[paper]](https://aclanthology.org/2023.acl-long.327/)
- `[Wang et al. ARXIV]` Dual-view Curricular Optimal Transport for Cross-lingual Cross-modal Retrieval. [[paper]](https://arxiv.org/pdf/2309.05451.pdf)
- `[Rouditchenko et al. ICASSP]` C2KD: Cross-Lingual Cross-Modal Knowledge Distillation for Multilingual Text-Video Retrieval. [[paper]](https://arxiv.org/abs/2210.03625) [[code]](https://github.com/roudimit/c2kd)
### 2022
- `[Wang et al. ACM MM]` Cross-Lingual Cross-Modal Retrieval with Noise-Robust Learning. [[paper]](https://arxiv.org/abs/2208.12526)  [[code]](https://github.com/HuiGuanLab/nrccr)
### 2021
- `[Zhou et al. CVPR21]` UC2:Universal Cross-lingual Cross-modal Vision-and-Language Pre-training. [[paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhou_UC2_Universal_Cross-Lingual_Cross-Modal_Vision-and-Language_Pre-Training_CVPR_2021_paper.pdf)  [[code]](https://github.com/zmykevin/UC2)
- `[Ni et al. CVPR21]` M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training. [[paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Ni_M3P_Learning_Universal_Representations_via_Multitask_Multilingual_Multimodal_Pre-Training_CVPR_2021_paper.pdf)  [[code]](https://github.com/microsoft/M3P)
- `[Huang et al. NAACL21]` Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models. [[paper]](https://arxiv.org/abs/2103.08849) 
- `[Fei et al. NAACL21]` Cross-lingual Cross-modal Pretraining for Multimodal Retrieval. [[paper]](https://aclanthology.org/2021.naacl-main.285.pdf)

### 2020
- `[Aggarwal et al. ARXIV]` Towards Zero-shot Cross-lingual Image Retrieval. [[paper]](https://arxiv.org/abs/2012.05107)

### 2019
- `[Portaz et al. ARXIV]` Image search using multilingual texts: a cross-modal learning approach between image and text. [[paper]](https://arxiv.org/abs/1903.11299)

### Chinese Cross-modal Pre-training
- `[Gu et al. NIPS22]` Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training Benchmark. [[paper]](https://proceedings.neurips.cc/paper_files/paper/2022/file/a90b9a09a6ee43d6631cf42e225d73b4-Paper-Datasets_and_Benchmarks.pdf) [[code]](https://wukong-dataset.github.io/wukong-dataset/)
- `[Xie et al. ARXIV22]` ZERO and R2D2: A Large-scale Chinese Cross-modal Benchmark and a Vision-Language Framework. [[paper]](https://arxiv.org/pdf/2205.03860.pdf) [[code]](https://github.com/yuxie11/R2D2)

